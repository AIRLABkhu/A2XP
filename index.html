<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="A2XP: Towards Private Domain Generalization">
  <meta property="og:title" content="A2XP: Towards Private Domain Generalization" />
  <meta property="og:description" content="A2XP: Towards Private Domain Generalization" />
  <meta property="og:url" content="https://airlabkhu.github.io/A2XP/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/fig1.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="A2XP: Towards Private Domain Generalization">
  <meta name="twitter:description" content="A2XP: Towards Private Domain Generalization">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/fig1.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>A2XP: Towards Private Domain Generalization</title>
  <link rel="icon" type="image/x-icon" href="static/images/air_jordan.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A2XP:<br />Towards Private Domain Generalization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=oXoHwFMAAAAJ&hl=en" target="_blank">Geunhyeok Yu</a> and
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=eQOlM-EAAAAJ&hl=en" target="_blank">Hyoseok Hwang</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">AIRLab, Kyung Hee University, Republic of Korea<br><strong>CVPR 2024</strong></span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <!-- <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" --> <!-- TODO: -->
                  <a href="static/pdfs/CVPR_2024_A2XP_Towards_Private_Domain_Generalization.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block"> <!-- TODO: -->
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank" 
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/AIRLABkhu/A2XP" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2311.10339" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser image -->
  <section class="hero teaser">
    <div class="container is-max-desktop">

      <div class="hero-body" style="text-align: center;">
        <figure style="text-align: center;">
          <img src="static/images/fig4.png" alt="fail">
          <figcaption class="content has-text-left"  style="word-break:normal">
            <b>Figure 1.</b> Comparison between A2XP (top) and non-private domain generalization method (bottom). 
            Non-private methods have to change the objective network's architecture or, at least, parameters while A2XP imployed visual prompts that keep the objective network private. 
          </figcaption>
        </figure>
        
        <div class="has-text-left" style="margin-top: 2em;">
          <h5 class="title is-5" style="margin-bottom: 1em;">Contributions</h5>        
          <ol style="padding-left: 2.4em; list-style-type:circle;">
            <li style="padding-bottom: 0.3em;"> Inspired by VP, introduce A2XP, which is a novel and simple domain generalization method that protects privacy.</li>
            <li style="padding-bottom: 0.3em;"> Mathematically analyze the generalization issue as an optimization of a linear combination problem.</li>
            <li style="padding-bottom: 0.3em;"> Further demonstrate the effectiveness and characteristics of A2XP and its component factors through extensive experiments.</li>
            <li style="padding-bottom: 0.3em;"> A2XP achieves SOTA over existing non-private domain generalization methods with significantly lower computational resource requirements.</li>
          </ol>
        </div>
      </div>

    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Deep Neural Networks (DNNs) have become pivotal in various fields, especially in computer vision, outperforming previous methodologies. 
              A critical challenge in their deployment is the bias inherent in data across different domains, such as image style and environmental conditions, leading to domain gaps. 
              This necessitates techniques for learning general representations from biased training data, known as domain generalization. 
              This paper presents Attend to eXpert Prompts (A2XP), a novel approach for domain generalization that preserves the privacy and integrity of the network architecture. A2XP consists of two phases: Expert Adaptation and Domain Generalization. 
              In the first phase, prompts for each source domain are optimized to guide the model towards the optimal direction. 
              In the second phase, two embedder networks are trained to effectively amalgamate these expert prompts, aiming for an optimal output. 
              Our extensive experiments demonstrate that A2XP achieves state-of-the-art results over existing non-private domain generalization methods.
              The experimental results validate that the proposed approach not only tackles the domain generalization challenge in DNNs but also offers a privacy-preserving, efficient solution to the broader field of computer vision.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Method carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Attend to Expert Prompts</h2>
        <div id="results-carousel" class="carousel results-carousel" style="text-align: center;">

          <!-- Geometric concept -->
         <div class="item" style="height: 100%; display: flex; flex-direction: column; justify-content: space-between; align-items: center;"> 
          <img src="static/images/fig3_.png" alt="fail" width="48%" style="margin-top: 3%;">
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Figure 2.</b> Geometric concept of A2XP as a linear combination in 2D manifold space with two source domains. 
            A2XP assumes that the generalization area (<b style="color: #7030A0">purple</b>) in the manifold space is reachable via linear combination of adaptation area (<b style="color: #548235">green</b> and <b style="color: #2F5597">blue</b>).
          </h2>
        </div>
        
        <!-- Flow diagram -->
        <div class="item"> 
          <img src="static/images/fig1.png" alt="fail" >
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Figure 3.</b> Flow diagram of A2XP. Our method first adapt the expert prompt for each domain. Then, combine them into a single prompt through attention-based manner. 
          </h2>
        </div>
        
        <!-- Inference stage -->
        <div class="item" style="height: 100%; display: flex; flex-direction: column; justify-content: space-between; align-items: center;"> 
          <img src="static/images/fig2.png" alt="fail" style="margin-top: 2.1%;">
          <h2 class="subtitle has-text-left" style="padding-bottom: 1%; font-size: 12pt">
            <b>Figure 4.</b> Inference procedure of A2XP. There are experts from source domains and target images of an unseen target domain. The experts are image-dependently mixed through an attention-based algorithm and added to the specific image.
          </h2>
        </div>
    </div>
  </div>
  </div>
  </section>
  <!-- End method carousel -->

  <!-- Experimental results -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Experimental Results</h2>

        <h5 class="title is-5">Target & Source Domain Evaluation</h3> 
        <p style="margin-bottom:2em;">
          Leave-one-domain-out evaluation and source domain evaluation was conducted to assess the generalizability of A2XP the results of which are detailed in Table 1.
          See Table 1a. 
          The five baselines were augmented using DART, which is an ensemble learning-based method for domain generalization. 
          A2XP outperformed all other methods in each target domain on both PACS and VLCS datasets. 
          It is important to mention that DART does not ensure the privacy of the objective network.
          Evaluation on all source domains well performed as much as on the target domain as shown in Table 1b.
        </p>
        <figure style="text-align: center; margin-bottom: 1em;">
          <img src="static/tables/tab1a.png" alt="fail">
          <figcaption class="content has-text-centered" style="word-break:normal">
            <b>(a)</b> Comparison with other methods in the target domain. DART was applied to the baselines for their best performance.
          </figcaption>
        </figure>
        <figure style="text-align: center; margin-bottom: 1em;">
          <img src="static/tables/tab1b.png" alt="fail">
          <figcaption class="content has-text-centered" style="word-break:normal">
            <b>(b)</b> Source domain evaluation on PACS (left) and VLCS (right) datasets.
          </figcaption>
        </figure>
        <figcaption class="content has-text-left" style="word-break:normal; margin-bottom: 2em;">
          <b>Table 1.</b> Target domain and source domain evaluations. Target domain evaluation was conducted to compare A2XP with other state-of-the-art methods. Source domain evaluation was conducted to see if it is still effective in the source domains.
        </figcaption>

        <h5 class="title is-5">Effectiveness of A2XP Module</h3> 
        <p style="margin-bottom:2em;">
          Ablation study was performed to demonstrate the efficacy of the A2XP module by quantifying its impact on accuracy enhancement with commonly used fine-tuning approaches such as linear probing and full tuning.
          As shown in Table 2, tuning the hidden layers appeared to impact the tuning of the output layer negatively. With the integration of A2XP in linear probing, accuracy was significantly increased across all tested domains. 
          However, in the case of full tuning, the inclusion of A2XP was counterproductive. 
          We analyzed that full tuning is inherently unstable; thus, the A2XP module, positioned before the hidden layers, was adversely affected.
        </p>
        <figure style="text-align: center; margin-bottom: 1em;">
          <img src="static/tables/tab2.png" alt="fail">
          <figcaption class="content has-text-centered" style="word-break:normal; margin-bottom: 2em;">
            <b>Table 2.</b> Comparison of tuning range on the objective network with and without A2XP. FT and LP refer to Full Tuning and Linear Probing, respectively.
          </figcaption>
        </figure>

        <h5 class="title is-5">Harmonized Domain Gathering</h3> 
        <p style="margin-bottom:2em;">
          The manifold space of the features extracted from the last hidden layer was visualized.
          As shown in Figure 5, to observe how the classes and domains are represented in a 2-dimensional space.
          Figure 5a-5d shows generalized features are mapped similarly regardless of the target domain.
        </p>
        <figure style="text-align: center; margin-bottom: 1em;">
          <img src="static/images/fig5.png" alt="fail">
          <figcaption class="content has-text-left" style="word-break:normal; margin-bottom: 2em;">
            <b>Figure 5.</b> t-SNE visualization of correctly classified samples in manifold space. (a)-(d) illustrate the representation achieved through generalization, with Picture, Art Painting, Cartoon, and Sketch as the target domains. (e) depicts the representation of expert adaptation prior to the generalization process.
          </figcaption>
        </figure>

        <h5 class="title is-5">Activation Gain & Loss</h3> 
        <p style="margin-bottom:2em;">
          To help understand the effects of A2XP on the neural network's focus, we visualized the activation maps. 
          Table 6 demonstrates that while linear probing is generalized in a way, the takes the generalizability even further. 
          This suggests that linear probing without A2XP yields reasonably effective activation maps, and the incorporation of A2XP further refines and improves these activation maps.
          The prompts shown in the (b) row change the activation maps as much as shown in (c) and (d). 
          The prompts have similar expression because they are from the same experts, but the intensities are different or some of them seem inverted. 
          This means the experts are mixed in different ratios dependent on the target image. 
          They show that A2XP makes the network attend more to the face representation and kills activation on other representations, such as the backgrounds or the body of an animal. 
          Specifically in the Picture domain, (c) shows that it primarily activates the ears of the dog and deactivates the background. 
          In the Sketch domain, it activates representations around the head while it deactivates the background next to the neck and the body, which contains fewer domain-agnostic clues for classification. 

        </p>
        <figure style="text-align: center; margin-bottom: 1em;">
          <img src="static/images/fig6.png" alt="fail" width="90%">
          <figcaption class="content has-text-left" style="word-break:normal;">
            <b>Figure 6.</b> Activation visualization of A2XP using Grad-CAM. (a) shows the input image, (c) and (d) show the relative gain and loss of activation using A2XP prompts in (b), respectively.
          </figcaption>
        </figure>

      </div>
    </div>
  </section>
  <!-- End experimental results -->

  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        To be presented.
        <!-- <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe> -->

      </div>
    </div>
  </section>
  <!--End paper poster -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@InProceedings{Yu_2024_CVPR,
  title={A2XP: Towards Private Domain Generalization},
  author={Yu, Geunhyeok and Hwang, Hyoseok},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  month={June}
  year={2024}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>